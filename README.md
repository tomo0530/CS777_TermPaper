# Chi Squared Feature Selection over Apache Spark

The inspiration for this term paper comes from the thesis by Dr. Mohamed Nassar. Apache Spark is a popular framework in the field of Big Data that has excelled in many computational tasks. However, a lousy implementation may lead to a significant decrease in performance and a waste of cluster time and money. Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to reduce the computational cost of modeling and, in some cases, improve the model's performance. In this paper, I consider the use case of Chi-Square from univariate feature selection, which is popular in supervised machine learning pipelines. They use partially different algorithms in scikit-learn machine learning and Spark ML, and I am interested in how this difference affects performance in other data volumes and execution environments. I also propose a new implementation using sparsity and verify its performance. 
I have experimented with datasets of different properties such as size, number of samples, number of features, and sparsity over the Databricks platform with Single Node cluster and Multi Node cluster. It showed that the new approach runs much faster than the current Spark ML to big data. However, the distributed processing on multiple nodes could not be taken advantage of, resulting in poor performance.
